{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a59276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d532e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "test_representation = []\n",
    "valid_representation = []\n",
    "train_representation = []\n",
    "\n",
    "label = [\"Black_Hair\", \"Blond_Hair\"]\n",
    "a = [\"5_o_Clock_Shadow\",\"Arched_Eyebrows\", \"Attractive\", \"Bags_Under_Eyes\", \"Bald\", \"Bangs\", \"Big_Lips\",\n",
    "         \"Big_Nose\", \"Black_Hair\", \"Blond_Hair\", \"Blurry\", \"Brown_Hair\", \"Bushy_Eyebrows\", \"Chubby\", \"Double_Chin\",\n",
    "         \"Eyeglasses\", \"Goatee\", \"Gray_Hair\", \"Heavy_Makeup\", \"High_Cheekbones\", \"Male\", \"Mouth_Slightly_Open\",\n",
    "         \"Mustache\", \"Narrow_Eyes\", \"No_Beard\", \"Oval_Face\", \"Pale_Skin\", \"Pointy_Nose\", \"Receding_Hairline\",\n",
    "         \"Rosy_Cheeks\", \"Sideburns\", \"Smiling\", \"Straight_Hair\", \"Wavy_Hair\", \"Wearing_Earrings\", \"Wearing_Hat\",\n",
    "         \"Wearing_Lipstick\", \"Wearing_Necklace\", \"Wearing_Necktie\", \"Young\"]\n",
    "\n",
    "for j in range(len(label)):\n",
    "    \n",
    "    hasf = open(\"./CelebA_Anno/\" + label[j] +\".txt\")\n",
    "\n",
    "    test_features = []\n",
    "    valid_features = []\n",
    "    train_features = []\n",
    "\n",
    "    for i in range(0, 500):\n",
    "        imgName = str(hasf.readline().split())\n",
    "        image = preprocess(Image.open(\"./CelebA/Img/img_align_celeba/\" + imgName)).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            test_features.append(image_features)\n",
    "    \n",
    "    for k in range(0, 500):\n",
    "        imgName = str(hasf.readline().split())\n",
    "        image = preprocess(Image.open(\"./CelebA/Img/img_align_celeba/\" + imgName)).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            valid_features.append(image_features)\n",
    "    \n",
    "    imgName = str(hasf.readline().split())\n",
    "    while imgName:\n",
    "        image = preprocess(Image.open(\"./CelebA/Img/img_align_celeba/\" + imgName)).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            train_features.append(image_features)\n",
    "        \n",
    "        imgName = str(hasf.readline().split())\n",
    "\n",
    "    features0 = torch.cat(test_features).cpu().numpy()\n",
    "    features1 = torch.cat(valid_features).cpu().numpy()\n",
    "    features2 = torch.cat(trian_features).cpu().numpy()\n",
    "    test_representation.append(features0)\n",
    "    valid_representation.append(features1)\n",
    "    train_representation.append(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayerWithActivation(nn.Module):\n",
    "    def __init__(self, input_shape, num_units, bias=False, activation_type=nn.ReLU()):\n",
    "        super(LinearLayerWithActivation, self).__init__()\n",
    "        self.activation_type = activation_type\n",
    "        self.weights = nn.Parameter(torch.empty(size=(num_units, input_shape[1]), requires_grad=True))\n",
    "        \n",
    "        nn.init.normal_(self.weights)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(num_units), requires_grad=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.linear(x, self.weights, self.bias)\n",
    "        out = self.activation_type.forward(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "eps = sys.float_info.epsilon\n",
    "\n",
    "k = 40 # categories\n",
    "\n",
    "fcc_net = LinearLayerWithActivation(input_shape= x.shape, num_units=k, bias=True, activation_type=nn.Identity())\n",
    "optimizer = optim.Adam(fcc_net.parameters(), amsgrad=False, weight_decay=0.0)\n",
    "\n",
    "\n",
    "for name, params in fcc_net.named_parameters():\n",
    "    print('Parameters with name', name, 'and shape', params.shape)\n",
    "\n",
    "metric_dict = {'losses1': [], 'losses2':[],'losses':[]}    \n",
    "    \n",
    "epochs = 100\n",
    "    \n",
    "for epoch in epochs:\n",
    "    batch_num0 = len(train_representation[0])//128\n",
    "    batch_num1 = len(train_representation[1])//128\n",
    "    num = min(batch_num0, batch_num1)\n",
    "    \n",
    "    for i in range(0, num):\n",
    "        x = torch.from_numpy(train_representation[i*128:(i+1)*128])\n",
    "        y = torch.from_numpy(train_representation[i*128:(i+1)*128])\n",
    "\n",
    "        out1 = fcc_net.forward(x)\n",
    "        out2 = fcc_net.forward(y)\n",
    "\n",
    "        v_x_pos = torch.var(out1, axis = 0)[0]\n",
    "        v_x_neg = torch.var(out1, axis = 0)[1]\n",
    "        v_y_pos = torch.var(out2, axis = 0)[0]\n",
    "        v_y_neg = torch.var(out2, axis = 0)[1]\n",
    "        loss1 = log(eps + v_x_pos) - log(eps + v_x_neg)\n",
    "        loss2 = log(eps + v_y_pos) - log(eps + v_y_neg)\n",
    "        loss = log(eps + v_x_pos) + log(eps + v_y_pos) - log(eps + v_x_neg) - log(eps + v_y_neg)\n",
    "\n",
    "        fcc_net.zero_grad() #removes grads of previous step\n",
    "        optimizer.zero_grad() #removes grads of previous step\n",
    "        loss.backward() #compute gradients of current step\n",
    "        optimizer.step() #update step\n",
    "        metric_dict['losses1'].append(loss1.detach().cpu().numpy()) #.detach: Copies the value of the loss \n",
    "    #                                                               and removes it from the graph, \n",
    "    #                                                             .cpu() sends to cpu, and \n",
    "    #                                                              numpy(), converts it to numpy format.\n",
    "        metric_dict['losses2'].append(loss2.detach().cpu().numpy()) \n",
    "        metric_dict['losses'].append(loss.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats_in_graph(metric_dict, y_axis_label='Loss', x_axis_label='Number of Steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81eedcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "l = [[1,2],[3,4],[5,6],[7,8]]\n",
    "a = torch.from_numpy(np.array(l[0:2]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfd8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
